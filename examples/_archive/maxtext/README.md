# Maxtext examples

This folder contains examples of pretraining  [Maxtext LLM](https://github.com/google/maxtext) using [C4 Dataset](https://www.tensorflow.org/datasets/catalog/c4). 

The `single_slice` folder is a Kustomize **overlay** over the **base** Job specification for a single slice training workload. The `multi_slice` folder is an **overlay** over the **base** JobSet specification for a multi-slice training workload.

Before configuring and running **Maxtext** training jobs you need to need download the C4 dataset and build a training container image with the Maxtext code base.

## Download C4 Dataset

Set PROJECT_ID and GCS_BUCKET to your project ID and a GCS bucket created during setup.

```
PROJECT_ID=jk-mlops-dev
GCS_BUCKET=gs://jk-gke-aiml-repository
DATASET_LOCATION="$GCS_BUCKET/datasets/c4/en/3.0.1"

gsutil -u $PROJECT_ID -m cp gs://allennlp-tensorflow-datasets/c4/en/3.0.1/* $DATASET_LOCATION

```

## Build Maxtext training container image

The `build.yaml` file is a **Cloud Build** configuration that automates a process of packaging the Maxtext code base into a docker container image and pushing it to your project's **Container Registry**

```
CLOUD_BUILD_REGION=us-central1
MAXTEXT_IMAGE_NAME=maxtext-runner-image

gcloud builds submit \
--project $PROJECT_ID \
--region $CLOUD_BUILD_REGION \
--substitutions _MAXTEXT_IMAGE_NAME=$MAXTEXT_IMAGE_NAME \
--config build.yaml \
--machine-type=e2-highcpu-32
```

## Maxtext training parameters

The Maxtext training loop is implemented in the [MaxText/train.py script](https://github.com/google/maxtext/blob/main/MaxText/train.py). The script accepts a number of command line parameters that control training settings. The first parameter is mandatory. It is a path to a YAML configuration file that contains model architecture and training regimen settings. The [MaxText/configs/base.yaml](https://github.com/google/maxtext/blob/main/MaxText/configs/base.yml) file contains the default settings. The other command line parameters are optional and can be used to overwrite settings in the YAML configuration file. You configure your training run by adapting a configuration file and/or providing command line overwrites.

In our samples, we use the default [MaxText/configs/base.yaml](https://github.com/google/maxtext/blob/main/MaxText/configs/base.yml) and configure a given run through command line overwrites.

For details on all configuration settings, refer to the [Maxtex repo](https://github.com/google/maxtext). Below, we provide a brief description of parameters used in the samples:
  - `run_name`. This is an identifier of your run. It is used to locate and store artifacts (including checkpoints) generated during training. They will be stored in the `run_name` folder in the `base_output_directory` path (see below). If there is an existing checkpoint in this location that checkpoint will auto-resume.    
  - `base_output_directory`. This a base GCS path for storing artifacts generated during runs.
  - `dataset_path`. This is the GCS location of the C4 dataset. This should be a GCS URI up to but not including the `c4` folder.
  - `steps`. The number of steps for this training run. If you do not set it, the default (as defined in `MaxText/configs/base.yml`) is 150,000.
  - `ici_fsdp_parallelism`. This parameter controls the ICI Fully Sharded Data Parallelism (FSDP) sharding strategy. For the Maxtext samples in this repo, it is recommended to set it to a number of chips in a TPU slice.
  - `dcn_data_parallelism`. This parameter controls the DCN Data Parallelism (DP) sharding strategy. For the Maxtext samples in this repo, it is recommended to set it to a number of slices.
  - The defaults in `MaxText/configs/base.yml` configure a model with 16 decoder layers, 8 attention heads per layer, and 2560 model dimension. If you would like to use a different model archicture adjust the following parameters: `base_emb_dim, base_num_heads, base_mlp_dim, base_num_decoder_layers, head_dim`:   

## Run a single-slice training job

Modify the `single_slice/job-spec-patch.yaml` file to reflect your environment:
- `metadata.name` - a unique job name. 
- `spec.parallelism` - a number of nodes in a multi-host TPU node pool
- `spec.template.spec.nodeSelector` - a TPU slice type (v4 or v5e) and TPU topology
- `spec.template.spec.containers[name=tpu-job].image` - your training image name
- `spec.template.spec.containers[name=tpu-job].command` - Maxtext training loop configuration. At minimum you need to update:
  - `run_name` - a unique run identifier. Altough not required, it is convenient to use the job name.
  - `dataset_path` - your GCS location of the C4 dataset
  - `base_output_directory` - your base output directory 

After updating the `job-spec-patch.yaml` you can submit the job using the following command:

```
kubectl apply -k single_slice
```

You can monitor the job by retrieving logs generated by any worker.

First, list all pods started by the job

```
kubectl get pods -n <YOUR TPU TRAINING NAMESPACE>
```

Pick  any pod in your job and copy its ID. Retrieve the logs for this pod.

```
kubectl logs <YOUR POD ID> -n <YOUR TPU TRAINING NAMESPACE>
```

You can also monitor the job using GCP Console.

## Run a multi-slice training job

If you want to run a multi-slice training job you need at least two TPU slices - a cluster provisioned with at least two multi-host TPU node pools.

Modify the `multi_slice/jobset-spec-patch.yaml` file to reflect your environment:
- `metadata.name` - a unique job set name.
- `spec.replicatedJobs.0.replicas` - a number of slices (node pools) to use for the job 
- `spec.replicatedJobs.0.template.spec.parallelism` - a number of nodes in a multi-host TPU node pool
- `spec.replicatedJobs.0.template.spec.nodeSelector` - a TPU slice type (v4 or v5e) and TPU topology
- `spec.replicatedJobs.0.template.spec.containers[name=tpu-job].image` - your training image name
- `spec.treplicatedJobs.0.template.spec.containers[name=tpu-job].command` - Maxtext training loop configuration. At minimum you need to update:
  - `run_name` - a unique run identifier. Altough not required, it is convenient to use the job name.
  - `dataset_path` - your GCS location of the C4 dataset
  - `base_output_directory` - your base output directory 
  - `ici_fsdp_parallelism` - set to a number of chips in a TPU slice (a multi-host TPU node pool)
  - `dcn_data_parallelism` - set to a number of TPU slices (a number of multi-host TPU node pools)

After updating the `jobset-spec-patch.yaml` you can submit the job using the following command:

```
kubectl apply -k multi_slice
```

You can monitor the job by retrieving logs generated by any worker.

First, list all pods started by the job

```
kubectl get pods -n <YOUR TPU TRAINING NAMESPACE>
```

Pick  any pod in your job and copy its ID. Retrieve the logs for this pod.

```
kubectl logs <YOUR POD ID> -n <YOUR TPU TRAINING NAMESPACE>
```

You can also monitor the job using GCP Console.


## Monitoring training runs using  Tensorboard

TBD
